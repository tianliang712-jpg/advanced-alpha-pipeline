# -*- coding: utf-8 -*-
# advanced_alpha_pipeline.py
# å•æ–‡ä»¶å…¨é›†æˆï¼ˆåœ¨æ­¤å‰ç‰ˆæœ¬â€œåªå¢ä¸å‡â€ï¼‰ï¼š
# TuShareä¼˜å…ˆ + AKShareå…œåº•(>=1å¹´å†å²) + è®­ç»ƒ/é¢„æµ‹/æ—¥å¿—/å›¾è¡¨/HTMLæŠ¥å‘Š
# + è¡Œä¸š/æ¦‚å¿µå¼ºåº¦ + ä¸­æ€§åŒ–IC/ICIR + T+0/2/3/5æ—¥æ‰§è¡Œå›æµ‹ + é˜ˆå€¼æœç´¢ + æ›´ç²¾ç»†æ¶¨è·Œåœçº¦æŸ
# + æ¶¨åœå¯è¾¾æ€§å­æ¨¡å‹ + CatBoost/æ·±åº¦å­¦ä¹ ï¼ˆPyTorch-MLPï¼‰å¯é€‰é›†æˆ
# + Streamlit å‰å°çœ‹æ¿ï¼ˆå«ï¼šIC/ICIRã€ICæ—¶é—´åºåˆ—ã€è¡Œä¸š/æ¦‚å¿µTop5ã€æ‰§è¡Œå±‚å¤šå‘¨æœŸå±•ç¤ºï¼‰
import os, warnings, base64, math, sys
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from datetime import datetime, timedelta

# ---- ç¬¬ä¸‰æ–¹åŒ… ----
import tushare as ts
import akshare as ak

from sklearn.linear_model import LinearRegression, Ridge
import lightgbm as lgb
import xgboost as xgb

# å¯é€‰ï¼šCatBoost / PyTorch
try:
    from catboost import CatBoostRegressor
    CATBOOST_AVAILABLE = True
except Exception:
    CATBOOST_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except Exception:
    TORCH_AVAILABLE = False

import matplotlib.pyplot as plt
import seaborn as sns

# å¯é€‰ï¼šStreamlitï¼ˆæœªè£…ä¹Ÿèƒ½çº¯åå°è¿è¡Œï¼‰
try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except Exception:
    STREAMLIT_AVAILABLE = False

# ================= é…ç½® =================
TS_TOKEN = "452b80dde79861ca8a9cd5ae6bee2d7defc0d85b0225f56f73678902"  # ä½ çš„ TuShare Key
ts.set_token(TS_TOKEN)
pro = ts.pro_api(TS_TOKEN)

DATA_DIR = "output"
os.makedirs(DATA_DIR, exist_ok=True)

# è‡³å°‘1å¹´å†å²ï¼ˆé€‚å½“å¤šæŠ“ä¸€äº›ï¼‰
START_DATE = (datetime.today() - timedelta(days=400)).strftime("%Y%m%d")
TOPK = 10
ROLL_TRAIN_WINDOW = 120
RANDOM_STATE = 2025
NEW_STOCK_DAYS = 60        # ä¸Šå¸‚æœªæ»¡Næ—¥è¿‡æ»¤
UNIVERSE_SIZE = 600        # æ¯æ—¥è®­ç»ƒ/æ‰“åˆ†è‚¡ç¥¨æ•°é‡ï¼ˆå¯è°ƒå¤§ï¼‰

# æ‰§è¡Œ/å›æµ‹å‚æ•°ï¼ˆæ–°å¢ï¼šåŒ…å« T+0 ä¸ 5æ—¥ï¼‰
EXEC_HOLD_DAYS_LIST = [0, 2, 3, 5]
STOP_WIN = 0.10
STOP_LOSS = -0.05

# é˜ˆå€¼æœç´¢ç½‘æ ¼
SEARCH_TOPK_GRID = [5, 10, 20]
SEARCH_PROB_TH = [0.55, 0.60, 0.65, 0.70]

# ================= å·¥å…·å‡½æ•° =================
def today_str(): return datetime.today().strftime("%Y%m%d")

def img_to_base64(img_path):
    with open(img_path, "rb") as f:
        return base64.b64encode(f.read()).decode()

def ts_code_to_symbol(ts_code:str):
    """000001.SZ -> 000001ï¼›600519.SH -> 600519"""
    return ts_code.split(".")[0]

def is_chinext(ts_code:str)->bool:
    # åˆ›ä¸š/ç§‘åˆ›ï¼ˆ300/301/688ï¼‰20% æ¶¨è·Œå¹…
    code = ts_code.split(".")[0]
    return code.startswith("300") or code.startswith("301") or code.startswith("688")

# ========= æ›´ç²¾ç»†çš„æ¶¨è·Œåœçº¦æŸ =========
def limit_up_threshold_by_market_name(ts_code:str, market:str, name:str)->float:
    """
    çº¦æŸä¼˜å…ˆçº§ï¼š
    - ST: 5%
    - BJï¼ˆåŒ—äº¤æ‰€ï¼‰: 30%  ï¼ˆstock_basic.market == 'BJ' æ—¶ï¼‰
    - åˆ›ä¸š/ç§‘åˆ›: 20%ï¼ˆ300/301/688ï¼‰
    - å…¶ä»–Aè‚¡: 10%
    """
    try:
        nm = (name or "").upper()
        if "ST" in nm:  # *ST æˆ– ST*
            return 0.05
    except Exception:
        pass
    try:
        if str(market).upper() == "BJ":
            return 0.30
    except Exception:
        pass
    return 0.20 if is_chinext(ts_code) else 0.10

# ================= ä¿®å¤æ—§æ—¥å¿—å·¥å…· =================
def repair_log():
    log_file = os.path.join(DATA_DIR, "log.csv")
    if not os.path.exists(log_file):
        print("[INFO] log.csv ä¸å­˜åœ¨ï¼Œæ— éœ€ä¿®å¤")
        return
    df = pd.read_csv(log_file)
    for c in ["ret_pred", "strong_prob"]:
        if c not in df.columns:
            df[c] = np.nan
            print(f"[INFO] å·²ä¸º log.csv è¡¥å…… {c} åˆ—")
    before = len(df)
    df = df.drop_duplicates(subset=["ts_code", "next_day"])
    after = len(df)
    if after < before:
        print(f"[INFO] å·²åˆ é™¤é‡å¤è®°å½• {before - after} è¡Œ")
    df["next_day"] = df["next_day"].astype(str)
    df.to_csv(log_file, index=False, encoding="utf-8-sig")
    print(f"[INFO] log.csv ä¿®å¤å®Œæˆ -> {log_file}")

# ================= æ•°æ®å±‚ï¼ˆTuShareä¼˜å…ˆ + AKShareå…œåº•ï¼‰ =================
def fetch_stock_basic():
    return pro.stock_basic(exchange="", list_status="L",
                           fields="ts_code,symbol,name,area,industry,market,list_date")

def fetch_daily(ts_code, start_date, end_date):
    """
    ä¼˜å…ˆ TuShareï¼ˆå«å‰å¤æƒå› å­ä¿®æ­£ close/open/high/lowï¼‰ï¼Œå¤±è´¥/ä¸ºç©ºåˆ™ç”¨ AKShare(qfq) å…œåº•ã€‚
    """
    try:
        df = pro.daily(ts_code=ts_code, start_date=start_date, end_date=end_date)
        if df is not None and not df.empty:
            adj = pro.adj_factor(ts_code=ts_code, start_date=start_date, end_date=end_date)
            if adj is not None and not adj.empty:
                df = df.merge(adj, on=["ts_code","trade_date"], how="left")
                df = df.sort_values("trade_date")
                last_adj = df["adj_factor"].iloc[-1]
                for col in ["close","open","high","low"]:
                    if col in df.columns:
                        df[col] = df[col] * df["adj_factor"] / last_adj
                return df
            else:
                df = df.sort_values("trade_date")
                return df
    except Exception as e:
        print(f"[WARN] TuShareè·å–å¤±è´¥ {ts_code}: {e}")

    try:
        sym = ts_code_to_symbol(ts_code)
        ak_df = ak.stock_zh_a_hist(symbol=sym, period="daily",
                                   start_date=start_date, end_date=end_date,
                                   adjust="qfq")
        if ak_df is not None and not ak_df.empty:
            rename_map = {
                "æ—¥æœŸ":"trade_date","å¼€ç›˜":"open","æ”¶ç›˜":"close",
                "æœ€é«˜":"high","æœ€ä½":"low","æˆäº¤é‡":"vol","æˆäº¤é¢":"amount"
            }
            for k in rename_map:
                if k in ak_df.columns:
                    ak_df.rename(columns={k: rename_map[k]}, inplace=True)
            ak_df["ts_code"] = ts_code
            ak_df["trade_date"] = pd.to_datetime(ak_df["trade_date"]).dt.strftime("%Y%m%d")
            return ak_df[["ts_code","trade_date","open","high","low","close","vol","amount"]]
    except Exception as e:
        print(f"[ERROR] AKShareè·å–å¤±è´¥ {ts_code}: {e}")

    return pd.DataFrame()

# ================= å› å­å·¥ç¨‹ =================
def build_features(df):
    df = df.copy()
    df["ret_1"]  = df["close"].pct_change(1)
    df["ret_5"]  = df["close"].pct_change(5)
    df["ma_5"]   = df["close"].rolling(5).mean()
    df["bias_5"] = (df["close"] - df["ma_5"])/df["ma_5"]
    df["volatility_10"] = df["ret_1"].rolling(10).std()
    if "amount" in df.columns:
        df["ln_mv"] = np.log(df["amount"] + 1)
    else:
        df["ln_mv"] = np.log((df["close"]*df.get("vol",0)).fillna(0)+1)
    df["next_ret_1"] = df["close"].pct_change(-1)
    return df.dropna()

# ====== è¡Œä¸š/æ¦‚å¿µå¼ºåº¦ ======
def compute_industry_strength(panel_day:pd.DataFrame)->pd.DataFrame:
    use = panel_day.dropna(subset=["industry","ret_1"])
    if use.empty:
        return pd.DataFrame(columns=["ts_code","industry","ind_strength","ind_leader_strength"])
    grp = use.groupby("industry")["ret_1"]
    ind_strength = grp.mean().rename("ind_strength")
    leader_strength = grp.max().rename("ind_leader_strength")
    out = use[["ts_code","industry"]].drop_duplicates().merge(
        ind_strength, on="industry").merge(leader_strength,on="industry")
    return out

def fetch_ths_concepts_safe():
    try:
        return ak.stock_board_concept_name_ths()
    except Exception:
        return pd.DataFrame()

def fetch_ths_concept_members_safe(board:str):
    try:
        return ak.stock_board_concept_cons_ths(symbol=board)
    except Exception:
        return pd.DataFrame()

def ts_code_from_6_or_not(code: str) -> str:
    code = str(code)
    if code.startswith("6"):
        return f"{code}.SH"
    return f"{code}.SZ"

def compute_concept_strength_top5_for_day(date_str:str)->pd.DataFrame:
    concepts = fetch_ths_concepts_safe()
    if concepts.empty:
        return pd.DataFrame(columns=["concept","strength"])
    top_rows = concepts.head(60)
    rows = []
    for _, r in top_rows.iterrows():
        board = r.get("æ¿å—åç§°") or r.get("æ¦‚å¿µåç§°") or r.get("name")
        if not isinstance(board, str):
            continue
        cons = fetch_ths_concept_members_safe(board)
        if cons is None or cons.empty:
            continue
        code_col = "ä»£ç " if "ä»£ç " in cons.columns else ("è‚¡ç¥¨ä»£ç " if "è‚¡ç¥¨ä»£ç " in cons.columns else None)
        if code_col is None:
            continue
        codes = cons[code_col].astype(str).unique().tolist()[:25]
        rets = []
        for c in codes:
            ts_c = ts_code_from_6_or_not(c)
            d = pro.daily(ts_code=ts_c, start_date=date_str, end_date=date_str)
            if d is not None and not d.empty:
                d = d.sort_values("trade_date")
                r1 = d["close"].pct_change().iloc[-1] if len(d)>=2 else 0.0
                rets.append(r1)
        if rets:
            rows.append({"concept": board, "strength": float(np.mean(rets))})
    if not rows:
        return pd.DataFrame(columns=["concept","strength"])
    df = pd.DataFrame(rows).sort_values("strength", ascending=False).head(5)
    df.to_csv(os.path.join(DATA_DIR, "concept_top5.csv"), index=False, encoding="utf-8-sig")
    return df

# ====== ä¸­æ€§åŒ–IC/ICIR ======
def neutralize_series_by_industry_size(df:pd.DataFrame, ret_col="next_ret_1", size_col="ln_mv", ind_col="industry"):
    use = df.dropna(subset=[ret_col, size_col, ind_col]).copy()
    if use.empty: 
        return pd.Series([], dtype=float)
    ind_dum = pd.get_dummies(use[ind_col], prefix="ind", drop_first=True)
    X = pd.concat([ind_dum, use[[size_col]]], axis=1)
    y = use[ret_col].values
    model = Ridge(alpha=1.0, fit_intercept=True)
    model.fit(X, y)
    y_hat = model.predict(X)
    resid = y - y_hat
    out = pd.Series(resid, index=use.index, name="ret_neu")
    return out

def compute_ic_series(train_df: pd.DataFrame, feature_cols: list):
    if train_df.empty:
        return pd.DataFrame(columns=["trade_date","ic"]), np.nan, np.nan
    # ä¸ä¸»æµç¨‹ä¸€è‡´ï¼šä¸¤æ ‘ + ï¼ˆå¯é€‰ï¼‰CatBoost + ï¼ˆå¯é€‰ï¼‰MLP â†’ èåˆ
    X = train_df[feature_cols].values
    y = train_df["next_ret_1"].values

    preds_list = []

    reg1 = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=RANDOM_STATE)
    reg1.fit(X, y); preds_list.append(reg1.predict(X))

    reg2 = xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=RANDOM_STATE)
    reg2.fit(X, y); preds_list.append(reg2.predict(X))

    if CATBOOST_AVAILABLE:
        reg3 = CatBoostRegressor(
            depth=6, iterations=300, learning_rate=0.05, verbose=False, random_seed=RANDOM_STATE
        )
        reg3.fit(X, y)
        preds_list.append(reg3.predict(X))

    if TORCH_AVAILABLE:
        preds_list.append(train_torch_mlp_and_predict(X, y, X, epochs=30, hidden=64))

    stack = np.vstack(preds_list).T
    lin = LinearRegression().fit(stack, np.mean(stack, axis=1))
    score = lin.predict(stack)

    df = train_df.copy()
    df["score"] = score
    ics = []
    for d, g in df.groupby("trade_date"):
        if g["score"].nunique() <= 1 or g["next_ret_1"].nunique() <= 1:
            continue
        ics.append({"trade_date": d, "ic": g["score"].corr(g["next_ret_1"])})
    ic_df = pd.DataFrame(ics).sort_values("trade_date")
    if ic_df.empty:
        return ic_df, np.nan, np.nan
    ic_mean = float(ic_df["ic"].mean())
    ic_std  = float(ic_df["ic"].std(ddof=1)) if len(ic_df) > 1 else 0.0
    icir = (ic_mean/ic_std*np.sqrt(252)) if ic_std>0 else np.nan
    ic_df.to_csv(os.path.join(DATA_DIR,"ic_series.csv"), index=False, encoding="utf-8-sig")
    plt.figure(figsize=(9,4))
    plt.plot(pd.to_datetime(ic_df["trade_date"]), ic_df["ic"], marker="o", linewidth=1)
    plt.axhline(0, color="black", linestyle="--")
    plt.title("æ¯æ—¥ICï¼ˆè®­ç»ƒçª—å£å†…ï¼Œèåˆåˆ†æ•° vs æ¬¡æ—¥æ”¶ç›Šï¼‰")
    plt.tight_layout()
    plt.savefig(os.path.join(DATA_DIR,"ic_timeseries.png"))
    plt.close()
    return ic_df, ic_mean, icir

# ====== å¯é€‰ï¼šPyTorch è½»é‡ MLP å›å½’ ======
def train_torch_mlp_and_predict(X_train, y_train, X_pred, epochs=30, hidden=64):
    class MLP(nn.Module):
        def __init__(self, in_dim, hidden):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(in_dim, hidden),
                nn.ReLU(),
                nn.Linear(hidden, hidden),
                nn.ReLU(),
                nn.Linear(hidden, 1)
            )
        def forward(self, x): return self.net(x)

    device = torch.device("cpu")
    Xtr = torch.tensor(X_train, dtype=torch.float32, device=device)
    ytr = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32, device=device)
    Xte = torch.tensor(X_pred, dtype=torch.float32, device=device)

    model = MLP(Xtr.shape[1], hidden).to(device)
    opt = optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()

    model.train()
    for _ in range(epochs):
        opt.zero_grad()
        pred = model(Xtr)
        loss = loss_fn(pred, ytr)
        loss.backward()
        opt.step()

    model.eval()
    with torch.no_grad():
        yhat = model(Xte).cpu().numpy().ravel()
    return yhat

# ====== â€œæ¶¨åœå¯è¾¾æ€§â€å­æ¨¡å‹ ======
def train_untradable_model(train_df:pd.DataFrame, feature_cols:list):
    if train_df.empty: 
        return None
    df = train_df.copy()
    # ä½¿ç”¨æ›´ç²¾ç»†é˜ˆå€¼ï¼šåŸºäº market/name
    df["limit_th"] = df.apply(lambda r: limit_up_threshold_by_market_name(
        r["ts_code"], r.get("market",""), r.get("name","")
    ), axis=1)
    df["y_limit"] = (df["next_ret_1"] >= (df["limit_th"] - 0.005)).astype(int)
    if df["y_limit"].sum() == 0:
        return None
    clf = lgb.LGBMClassifier(n_estimators=150, learning_rate=0.05, random_state=RANDOM_STATE)
    X = df[feature_cols].fillna(0).values
    y = df["y_limit"].values
    clf.fit(X, y)
    return clf

# ====== ä¸»æ¨¡å‹/stackingï¼ˆå¯é€‰CatBoost/æ·±åº¦å­¦ä¹ ï¼‰ ======
def train_models_extended(X, y_class, y_reg):
    clf = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05, random_state=RANDOM_STATE)

    reg_list = []
    reg1 = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=RANDOM_STATE)
    reg1.fit(X, y_reg); reg_list.append(lambda Z: reg1.predict(Z))

    reg2 = xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=RANDOM_STATE)
    reg2.fit(X, y_reg); reg_list.append(lambda Z: reg2.predict(Z))

    if CATBOOST_AVAILABLE:
        reg3 = CatBoostRegressor(
            depth=6, iterations=300, learning_rate=0.05, verbose=False, random_seed=RANDOM_STATE
        )
        reg3.fit(X, y_reg)
        reg_list.append(lambda Z: reg3.predict(Z))

    if TORCH_AVAILABLE:
        # è®­ç»ƒä¸€æ¬¡ MLPï¼Œç”¨é—­åŒ…ç¼“å­˜æƒé‡åçš„é¢„æµ‹å‡½æ•°
        yhat = train_torch_mlp_and_predict(X, y_reg, X, epochs=30, hidden=64)
        # ä¸ºäº†ç®€å•ï¼Œè¿™é‡Œå†è®­ä¸€éè·å–é¢„æµ‹å™¨ï¼ˆé¿å…ä¿å­˜æ¨¡å‹æƒé‡åˆ°æ–‡ä»¶ï¼‰
        def torch_pred(Z):
            return train_torch_mlp_and_predict(X, y_reg, Z, epochs=10, hidden=64)
        reg_list.append(torch_pred)

    clf.fit(X, y_class)
    return clf, reg_list

def stacking_predict_extended(clf, reg_list, X):
    preds = [rfn(X) for rfn in reg_list]
    stack = np.vstack(preds).T
    lin = LinearRegression().fit(stack, np.mean(stack, axis=1))
    return clf.predict_proba(X)[:,1], lin.predict(stack)

# ====== åœ¨çº¿å¢é‡å­¦ä¹ &æ‰“åˆ† ======
def daily_update():
    basics = fetch_stock_basic()
    today = today_str()
    pred_file = os.path.join(DATA_DIR, f"predictions_{today}.csv")
    if os.path.exists(pred_file):
        print(f"[INFO] å·²æœ‰ {pred_file}ï¼Œè·³è¿‡")
        return None, None, None, None

    universe = basics.ts_code.head(UNIVERSE_SIZE).tolist()
    frames = []
    bidx = basics.set_index("ts_code")
    for code in universe:
        d = fetch_daily(code, START_DATE, today)
        if d.empty: continue
        f = build_features(d)
        if f.empty: continue
        f["ts_code"] = code
        # è¿½åŠ  name / marketï¼Œä¾›æ›´ç²¾ç»†æ¶¨è·Œåœä½¿ç”¨
        f["name"]   = bidx.loc[code,"name"] if "name" in bidx.columns else ""
        f["market"] = (bidx.loc[code,"market"] if "market" in bidx.columns else "").upper()
        f["industry"] = bidx.loc[code,"industry"] if "industry" in bidx.columns else "NA"
        list_date = bidx.loc[code,"list_date"]
        f["list_days"] = (pd.to_datetime(f["trade_date"]) - pd.to_datetime(list_date)).dt.days
        frames.append(f)
    if not frames: 
        return None, None, None, None

    panel = pd.concat(frames, ignore_index=True)

    # è¡Œä¸š/æ¦‚å¿µå¼ºåº¦
    feature_base = ["ret_1","ret_5","bias_5","volatility_10","ln_mv"]
    last_day = panel["trade_date"].max()
    day_cross = panel[panel["trade_date"]==last_day].copy()
    ind_strength_df = compute_industry_strength(day_cross)
    panel = panel.merge(ind_strength_df, on=["ts_code","industry"], how="left")
    for c in ["ind_strength","ind_leader_strength"]:
        panel[c] = panel[c].fillna(0.0)

    concept_top5 = compute_concept_strength_top5_for_day(last_day)
    concept_strength_val = float(concept_top5["strength"].mean()) if (concept_top5 is not None and not concept_top5.empty) else 0.0
    panel["concept_strength"] = concept_strength_val

    # è¡Œä¸šTop5è¾“å‡º
    if not day_cross.empty:
        tmp = panel[panel["trade_date"]==last_day].dropna(subset=["industry","ind_strength"])
        if not tmp.empty:
            ind_top5 = tmp.groupby("industry")["ind_strength"].mean().reset_index().sort_values("ind_strength", ascending=False).head(5)
            ind_top5.to_csv(os.path.join(DATA_DIR,"industry_top5.csv"), index=False, encoding="utf-8-sig")

    # ä¸­æ€§åŒ–
    panel["ret_neu"] = np.nan
    idx_mask = panel["trade_date"].isin(panel["trade_date"].unique()[-ROLL_TRAIN_WINDOW:])
    sub = panel[idx_mask].copy()
    resid = neutralize_series_by_industry_size(sub, ret_col="next_ret_1", size_col="ln_mv", ind_col="industry")
    panel.loc[sub.index, "ret_neu"] = resid

    # è®­ç»ƒ
    feature_cols = [c for c in feature_base+["ind_strength","ind_leader_strength","concept_strength"] if c in panel.columns]
    df_train = panel.dropna(subset=feature_cols+["next_ret_1"])
    if df_train.empty:
        print("[WARN] è®­ç»ƒæ•°æ®ä¸ºç©º")
        return None, None, None, None

    last_days = df_train.trade_date.unique()[-ROLL_TRAIN_WINDOW:]
    train = df_train[df_train.trade_date.isin(last_days)].copy()

    # ICåºåˆ— + IC/ICIR
    ic_df, ic_mean, icir = compute_ic_series(train, feature_cols)
    if ic_df is not None and not ic_df.empty:
        pd.DataFrame([{"ic_mean": ic_mean, "icir": icir, "window_days": len(ic_df)}]).to_csv(
            os.path.join(DATA_DIR,"ic_summary.csv"), index=False, encoding="utf-8-sig"
        )

    # ä¸»æ¨¡å‹ + å¯é€‰æ¨¡å‹
    clf, reg_list = train_models_extended(
        train[feature_cols].values,
        (train["next_ret_1"]>0.01).astype(int),
        train["next_ret_1"].values
    )

    # é¢„æµ‹æœ€æ–°äº¤æ˜“æ—¥
    pred = df_train[df_train.trade_date==last_day].copy()
    prob, preds = stacking_predict_extended(clf, reg_list, pred[feature_cols].values)
    pred["strong_prob"], pred["ret_pred_raw"] = prob, preds

    # å­æ¨¡å‹ï¼šæ¶¨åœå¯è¾¾æ€§ï¼ˆä½¿ç”¨æ›´ç²¾ç»†é˜ˆå€¼ï¼‰
    untrad_clf = train_untradable_model(train, feature_cols)
    if untrad_clf is not None:
        p_untrad = untrad_clf.predict_proba(pred[feature_cols].values)[:,1]
        pred["untrad_prob"] = p_untrad
        pred["ret_pred"] = pred["ret_pred_raw"] * (1 - pred["untrad_prob"])
    else:
        pred["untrad_prob"] = 0.0
        pred["ret_pred"] = pred["ret_pred_raw"]

    # çº¦æŸï¼šä¸Šå¸‚æœªæ»¡Næ—¥
    pred = pred[(pred["list_days"]>=NEW_STOCK_DAYS)]

    picks = pred.sort_values("ret_pred",ascending=False).head(TOPK)
    picks.to_csv(pred_file,index=False,encoding="utf-8-sig")
    print(f"[INFO] {today} é¢„æµ‹ç»“æœå·²ä¿å­˜ -> {pred_file}")
    return picks, last_day, ic_mean, icir

# ====== é¢„æµ‹ vs å®é™…æ—¥å¿— ======
def update_log(preds, pred_day):
    next_day = (pd.to_datetime(pred_day) + timedelta(days=1)).strftime("%Y%m%d")
    log_file = os.path.join(DATA_DIR,"log.csv")
    rets = []
    for code in preds.ts_code:
        df = fetch_daily(code,next_day,next_day)
        if df.empty: continue
        rets.append({
            "ts_code": code,
            "pred_day": pred_day,
            "next_day": next_day,
            "real_ret": df["close"].pct_change().iloc[-1] if len(df) > 1 else 0.0,
            "ret_pred": float(preds.loc[preds.ts_code==code,"ret_pred"].iloc[0]) if "ret_pred" in preds.columns else np.nan,
            "strong_prob": float(preds.loc[preds.ts_code==code,"strong_prob"].iloc[0]) if "strong_prob" in preds.columns else np.nan
        })
    if not rets: return
    df_log = pd.DataFrame(rets)

    if os.path.exists(log_file):
        old = pd.read_csv(log_file)
        for c in ["ret_pred","strong_prob"]:
            if c not in old.columns: old[c] = np.nan
        df_log = pd.concat([old,df_log], ignore_index=True)

    df_log.to_csv(log_file,index=False,encoding="utf-8-sig")
    print(f"[INFO] å®é™…æ”¶ç›Šå·²è®°å½• -> {log_file}")

# ====== æ‰§è¡Œå±‚å›æµ‹ï¼ˆå« T+0 / 2 / 3 / 5 æ—¥ï¼‰ ======
def simulate_execution_from_logs(hold_days=2, stop_win=STOP_WIN, stop_loss=STOP_LOSS):
    log_file = os.path.join(DATA_DIR,"log.csv")
    if not os.path.exists(log_file):
        print("[INFO] æ—  log.csvï¼Œè·³è¿‡æ‰§è¡Œå›æµ‹")
        return None
    df = pd.read_csv(log_file)
    if df.empty: return None

    day_rets = []
    for d, g in df.groupby("next_day"):
        rets = []
        for _, row in g.iterrows():
            code = row["ts_code"]
            start = (pd.to_datetime(row["next_day"], format="%Y%m%d")).strftime("%Y%m%d")
            end   = (pd.to_datetime(row["next_day"], format="%Y%m%d") + timedelta(days=max(hold_days-1,0))).strftime("%Y%m%d")
            k = fetch_daily(code, start, end)
            if k is None or k.empty:
                continue
            k = k.sort_values("trade_date")
            # T+0ï¼šå½“æ—¥å¼€ä¹°ï¼Œå½“æ—¥ç¦»åœºï¼ˆæˆ–æ­¢ç›ˆæ­¢æŸï¼‰
            if hold_days == 0:
                entry = float(k["open"].iloc[0]) if "open" in k.columns else float(k["close"].iloc[0])
                high  = float(k["high"].iloc[0]) if "high" in k.columns else float(k["close"].iloc[0])
                low   = float(k["low"].iloc[0])  if "low"  in k.columns else float(k["close"].iloc[0])
                exitp = float(k["close"].iloc[0])
                realized = None
                if (high - entry)/entry >= stop_win: realized = stop_win
                if realized is None and (low - entry)/entry <= stop_loss: realized = stop_loss
                if realized is None: realized = (exitp - entry)/entry
                rets.append(realized)
                continue

            # å¤šæ—¥ï¼šç¬¬ä¸€å¤©å¼€ç›˜ä¹°ï¼Œæ—¥å†…æ­¢ç›ˆ/æ­¢æŸï¼Œå¦åˆ™æœ€åä¸€å¤©æ”¶ç›˜ç¦»åœº
            entry_price = float(k["open"].iloc[0]) if "open" in k.columns else float(k["close"].iloc[0])
            exit_price  = float(k["close"].iloc[-1])
            realized = None
            for i in range(len(k)):
                high = float(k["high"].iloc[i]) if "high" in k.columns else float(k["close"].iloc[i])
                low  = float(k["low"].iloc[i])  if "low"  in k.columns else float(k["close"].iloc[i])
                if (high - entry_price) / entry_price >= stop_win:
                    realized = stop_win; break
                if (low - entry_price) / entry_price <= stop_loss:
                    realized = stop_loss; break
            if realized is None:
                realized = (exit_price - entry_price) / entry_price
            rets.append(realized)
        if rets:
            day_rets.append({"day": d, "ret": float(np.mean(rets))})
    if not day_rets:
        return None
    exec_df = pd.DataFrame(day_rets).sort_values("day")
    exec_df["cum"] = (1+exec_df["ret"]).cumprod()
    fn = os.path.join(DATA_DIR, f"execution_hold{hold_days}.png")
    plt.figure(figsize=(8,4))
    plt.plot(exec_df["cum"].values, label=f"æ‰§è¡Œå±‚å‡€å€¼(Hold{hold_days})")
    plt.legend(); plt.tight_layout(); plt.savefig(fn); plt.close()
    print(f"[INFO] æ‰§è¡Œå±‚æ›²çº¿å·²ä¿å­˜ -> {fn}")
    return exec_df

# ====== é˜ˆå€¼æœç´¢ï¼ˆåŸºäº log.csv å†å²ï¼‰ ======
def threshold_search_from_logs():
    log_file = os.path.join(DATA_DIR,"log.csv")
    if not os.path.exists(log_file):
        print("[INFO] æ—  log.csvï¼Œé˜ˆå€¼æœç´¢è·³è¿‡")
        return None
    df = pd.read_csv(log_file).dropna(subset=["real_ret"])
    if df.empty or "strong_prob" not in df.columns:
        print("[INFO] å†å²ä¸è¶³æˆ–ç¼º strong_probï¼Œé˜ˆå€¼æœç´¢è·³è¿‡")
        return None

    best = None
    for th in SEARCH_PROB_TH:
        for k in SEARCH_TOPK_GRID:
            day_port = []
            for d, g in df.groupby("pred_day"):
                gg = g.copy()
                gg = gg[gg["strong_prob"]>=th] if "strong_prob" in gg.columns else gg
                if gg.empty: 
                    continue
                gg = gg.sort_values("ret_pred", ascending=False).head(k) if "ret_pred" in gg.columns else gg.head(k)
                if gg.empty: 
                    continue
                day_port.append(gg["real_ret"].mean())
            if not day_port:
                continue
            r = np.array(day_port)
            ann_ret = r.mean()*252
            ann_vol = r.std()*np.sqrt(252) if r.std()>0 else np.nan
            sharpe = ann_ret/ann_vol if ann_vol and not np.isnan(ann_vol) else -9
            score = sharpe
            if (best is None) or (score > best["score"]):
                best = {"prob_th": th, "topk": k, "score": score, "ann_ret": ann_ret, "ann_vol": ann_vol}
    if best:
        print(f"[INFO] é˜ˆå€¼æœç´¢æœ€ä½³: prob>={best['prob_th']}, TOPK={best['topk']}, Sharpe={best['score']:.2f}")
        pd.DataFrame([best]).to_csv(os.path.join(DATA_DIR,"threshold_search.csv"), index=False, encoding="utf-8-sig")
    else:
        print("[INFO] é˜ˆå€¼æœç´¢æœªæ‰¾åˆ°å¯è¡Œè§£")
    return best

# ====== å›¾è¡¨/æŠ¥å‘Š ======
def generate_charts():
    log_file = os.path.join(DATA_DIR,"log.csv")
    if not os.path.exists(log_file):
        print("[INFO] æ—  log.csvï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        return
    df = pd.read_csv(log_file)
    if df.empty:
        print("[INFO] log.csv ä¸ºç©ºï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        return

    # ç´¯è®¡å‡€å€¼
    ret = df["real_ret"].fillna(0)
    cum = (1+ret).cumprod()
    plt.figure(figsize=(8,4))
    plt.plot(cum.values, label="ç­–ç•¥å‡€å€¼")
    plt.title("ç­–ç•¥ç´¯è®¡å‡€å€¼")
    plt.xlabel("äº¤æ˜“æ—¥åºå·")
    plt.ylabel("å‡€å€¼")
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(DATA_DIR,"performance.png"))
    plt.close()

    # æœˆåº¦çƒ­åŠ›å›¾
    df["year"] = df["next_day"].astype(str).str[:4]
    df["month"] = df["next_day"].astype(str).str[4:6].astype(int)
    heat = df.groupby(["year","month"])["real_ret"].mean().reset_index()
    if not heat.empty:
        pv = heat.pivot(index="year", columns="month", values="real_ret")
        plt.figure(figsize=(10,4))
        sns.heatmap(pv, annot=True, fmt=".2%", cmap="RdYlGn", center=0)
        plt.title("æœˆåº¦å¹³å‡æ”¶ç›Šçƒ­åŠ›å›¾")
        plt.tight_layout()
        plt.savefig(os.path.join(DATA_DIR,"monthly_heatmap.png"))
        plt.close()

    # åŸºå‡†å¯¹æ¯”ï¼šæ²ªæ·±300
    try:
        start = df["next_day"].min()
        end   = df["next_day"].max()
        bench = fetch_daily("000300.SH", str(start), str(end))
        if not bench.empty:
            b = bench.set_index("trade_date")["close"].pct_change().fillna(0)
            bc = (1+b).cumprod()
            idx = range(len(cum))
            plt.figure(figsize=(8,4))
            plt.plot(idx, cum.values, label="ç­–ç•¥å‡€å€¼")
            plt.plot(idx, bc.values[:len(idx)], label="æ²ªæ·±300")
            plt.title("ç­–ç•¥ vs æ²ªæ·±300")
            plt.xlabel("äº¤æ˜“æ—¥åºå·")
            plt.ylabel("å‡€å€¼")
            plt.legend()
            plt.tight_layout()
            plt.savefig(os.path.join(DATA_DIR,"strategy_vs_benchmark.png"))
            plt.close()
    except Exception as e:
        print(f"[WARN] åŸºå‡†å›¾å¤±è´¥: {e}")

def generate_html_report(extra_imgs=[]):
    html_file = os.path.join(DATA_DIR,"report.html")
    log_file = os.path.join(DATA_DIR,"log.csv")

    html = ["<html><head><meta charset='utf-8'><title>ç­–ç•¥æŠ¥å‘Š</title></head><body>"]
    html.append("<h1>ğŸ“Š ç­–ç•¥é¢„æµ‹æŠ¥å‘Š</h1>")

    # IC/ICIR è¡¨
    ic_sum_file = os.path.join(DATA_DIR, "ic_summary.csv")
    if os.path.exists(ic_sum_file):
        ic_sum = pd.read_csv(ic_sum_file)
        html.append("<h2>ğŸ” IC / ICIR æ¦‚è§ˆï¼ˆè®­ç»ƒçª—å£ï¼‰</h2>")
        html.append(ic_sum.to_html(index=False, float_format=lambda x: f"{x:.4f}" if isinstance(x, float) else str(x)))

    # å†å²æ—¥å¿—
    if os.path.exists(log_file):
        df = pd.read_csv(log_file)
        html.append("<h2>ğŸ“ˆ å†å²é¢„æµ‹æ—¥å¿—ï¼ˆæœ€è¿‘20æ¡ï¼‰</h2>")
        html.append(df.tail(20).to_html(index=False, escape=False))

    # å¸‚åœºçƒ­ç‚¹
    ind_top_path = os.path.join(DATA_DIR, "industry_top5.csv")
    con_top_path = os.path.join(DATA_DIR, "concept_top5.csv")
    if os.path.exists(ind_top_path) or os.path.exists(con_top_path):
        html.append("<h2>ğŸ”¥ å¸‚åœºçƒ­ç‚¹ï¼ˆTop5ï¼‰</h2><div style='display:flex;gap:32px;'>")
        if os.path.exists(ind_top_path):
            ind_top = pd.read_csv(ind_top_path)
            html.append("<div><h3>è¡Œä¸šå¼ºåº¦ Top5</h3>")
            html.append(ind_top.to_html(index=False, float_format=lambda x: f"{x:.2%}" if isinstance(x,float) else str(x)))
            html.append("</div>")
        if os.path.exists(con_top_path):
            con_top = pd.read_csv(con_top_path)
            html.append("<div><h3>æ¦‚å¿µå¼ºåº¦ Top5</h3>")
            html.append(con_top.to_html(index=False, float_format=lambda x: f"{x:.2%}" if isinstance(x,float) else str(x)))
            html.append("</div>")
        html.append("</div>")

    # åµŒå…¥å›¾ç‰‡ï¼ˆæ–°å¢ T+0 / 5æ—¥æ›²çº¿ + ICæ—¶é—´åºåˆ—ï¼‰
    img_list = [
        os.path.join(DATA_DIR,"ic_timeseries.png"),
        os.path.join(DATA_DIR,"performance.png"),
        os.path.join(DATA_DIR,"monthly_heatmap.png"),
        os.path.join(DATA_DIR,"strategy_vs_benchmark.png"),
        os.path.join(DATA_DIR,"execution_hold0.png"),
        os.path.join(DATA_DIR,"execution_hold2.png"),
        os.path.join(DATA_DIR,"execution_hold3.png"),
        os.path.join(DATA_DIR,"execution_hold5.png"),
    ] + list(extra_imgs or [])

    for img in img_list:
        if os.path.exists(img):
            html.append(f"<h3>{os.path.basename(img)}</h3>")
            html.append(f"<img src='data:image/png;base64,{img_to_base64(img)}' width='900'>")

    html.append("</body></html>")
    with open(html_file,"w",encoding="utf-8") as f:
        f.write("\n".join(html))
    print(f"[INFO] HTMLæŠ¥å‘Šå·²ç”Ÿæˆ -> {html_file}")

# ====== è·Ÿè¸ªå›¾ ======
def plot_stock_track(ts_code):
    log_file = os.path.join(DATA_DIR,"log.csv")
    if not os.path.exists(log_file): return None
    df = pd.read_csv(log_file)
    df_stock = df[df["ts_code"]==ts_code]
    if df_stock.empty: return None
    df_stock["date"] = pd.to_datetime(df_stock["next_day"], format="%Y%m%d")
    plt.figure(figsize=(8,4))
    if "ret_pred" in df_stock.columns:
        plt.plot(df_stock["date"], df_stock["ret_pred"], label="é¢„æµ‹æ”¶ç›Š", marker="o")
    plt.plot(df_stock["date"], df_stock["real_ret"], label="å®é™…æ”¶ç›Š", marker="x")
    plt.axhline(0,color="black",linestyle="--")
    plt.title(f"{ts_code} é¢„æµ‹ vs å®é™…æ”¶ç›Š")
    plt.xlabel("æ—¥æœŸ"); plt.ylabel("æ”¶ç›Šç‡")
    plt.legend(); plt.tight_layout()
    fig_file = os.path.join(DATA_DIR,f"track_{ts_code}.png")
    plt.savefig(fig_file); plt.close()
    print(f"[INFO] å•è‚¡è·Ÿè¸ªå›¾å·²ä¿å­˜ -> {fig_file}")
    return fig_file

def plot_topN_stocks(n=10):
    log_file = os.path.join(DATA_DIR,"log.csv")
    if not os.path.exists(log_file): return []
    df = pd.read_csv(log_file)
    if df.empty: return []
    top_stocks = df["ts_code"].value_counts().head(n).index.tolist()
    files = []
    for code in top_stocks:
        f = plot_stock_track(code)
        if f: files.append(f)
    return files

# ================= ä¸»æµç¨‹ï¼ˆåå°ï¼‰ =================
def run_backend_once():
    # 1) ä¿®å¤æ—¥å¿—
    repair_log()
    # 2) å¢é‡è®­ç»ƒ + å½“æ—¥é¢„æµ‹ï¼ˆå« IC/ICIR ä¸ çƒ­ç‚¹è¾“å‡ºï¼‰
    picks, last_day, ic_mean, icir = daily_update()
    if picks is not None:
        cols = [c for c in ["ts_code","strong_prob","ret_pred_raw","untrad_prob","ret_pred"] if c in picks.columns]
        print("ä»Šæ—¥é¢„æµ‹ TopKï¼š")
        print(picks[cols])
        # 3) å†™å…¥é¢„æµ‹ vs å®é™…æ—¥å¿—
        update_log(picks,last_day)
    # 4) æ‰§è¡Œå±‚å›æµ‹ï¼ˆT+0 / 2 / 3 / 5ï¼‰
    for d in EXEC_HOLD_DAYS_LIST:
        simulate_execution_from_logs(hold_days=d, stop_win=STOP_WIN, stop_loss=STOP_LOSS)
    # 5) é˜ˆå€¼æœç´¢
    threshold_search_from_logs()
    # 6) å›¾è¡¨
    generate_charts()
    # 7) è·Ÿè¸ªå›¾ï¼ˆTop10 é«˜é¢‘ï¼‰
    extra_imgs = plot_topN_stocks(10)
    # 8) HTML æŠ¥å‘Šï¼ˆå«ï¼šIC/ICIRè¡¨ã€ICæ—¶é—´åºåˆ—å›¾ã€è¡Œä¸š/æ¦‚å¿µTop5ã€æ‰§è¡Œå±‚å¤šå‘¨æœŸæ›²çº¿ï¼‰
    generate_html_report(extra_imgs)

# ================= å‰å°ï¼ˆStreamlitï¼‰ =================
def run_streamlit_ui():
    st.set_page_config(page_title="ç­–ç•¥çœ‹æ¿ï¼ˆå«å¤šå‘¨æœŸå›æµ‹ä¸å¯é€‰DL/CatBoostï¼‰", layout="wide")
    st.title("ğŸ“ˆ ç­–ç•¥äº¤äº’çœ‹æ¿")
    st.caption("TuShareä¼˜å…ˆ + AKShareå…œåº•ï¼›â‰¥1å¹´å†å²ï¼›è¡Œä¸š/æ¦‚å¿µå¼ºåº¦ã€IC/ICIRã€T+0/2/3/5å›æµ‹ã€å¯é€‰CatBoostä¸æ·±åº¦å­¦ä¹ å­æ¨¡å‹ã€‚")

    if st.sidebar.button("é‡æ–°è¿è¡Œåå°æµç¨‹ï¼ˆè®­ç»ƒ/é¢„æµ‹/æ›´æ–°æŠ¥è¡¨ï¼‰"):
        with st.spinner("æ­£åœ¨è¿è¡Œåå°æµç¨‹..."):
            run_backend_once()
        st.success("åå°æµç¨‹å®Œæˆï¼Œæ•°æ®å·²æ›´æ–°ã€‚")

    # è½½å…¥æ—¥å¿—
    log_file = os.path.join(DATA_DIR,"log.csv")
    if os.path.exists(log_file):
        df_log = pd.read_csv(log_file)
    else:
        df_log = pd.DataFrame()

    # è½½å…¥ IC æ±‡æ€»ä¸åºåˆ—
    ic_sum_file = os.path.join(DATA_DIR,"ic_summary.csv")
    ic_series_file = os.path.join(DATA_DIR,"ic_series.csv")
    ic_mean = icir = None
    if os.path.exists(ic_sum_file):
        ic_sum = pd.read_csv(ic_sum_file)
        if not ic_sum.empty:
            ic_mean = ic_sum["ic_mean"].iloc[0]
            icir = ic_sum["icir"].iloc[0]

    # æŒ‡æ ‡åŒº
    st.subheader("ğŸ“Š ç»©æ•ˆæ€»è§ˆ")
    if df_log.empty:
        st.info("æš‚æ— æ—¥å¿—æ•°æ®ã€‚è¯·å…ˆåœ¨ä¾§è¾¹æ è¿è¡Œåå°æµç¨‹æˆ–ç”¨ python æ–¹å¼è¿è¡Œæœ¬è„šæœ¬ã€‚")
    else:
        ret = df_log["real_ret"].fillna(0)
        win = (ret>0).mean(); avg = ret.mean()
        ann_ret = avg*252; ann_vol = ret.std()*np.sqrt(252)
        sharpe = ann_ret/ann_vol if ann_vol>0 else 0
        ic_val = df_log["ret_pred"].corr(df_log["real_ret"]) if "ret_pred" in df_log.columns else np.nan
        if ic_mean is not None and not np.isnan(ic_mean):
            ic_val = ic_mean
        rank_ic = df_log["ret_pred"].rank().corr(df_log["real_ret"].rank()) if "ret_pred" in df_log.columns else np.nan

        c1,c2,c3,c4,c5 = st.columns(5)
        c1.metric("èƒœç‡", f"{win:.2%}")
        c2.metric("å¹³å‡æ”¶ç›Š/æ—¥", f"{avg:.2%}")
        c3.metric("Sharpe", f"{sharpe:.2f}")
        c4.metric("IC", f"{ic_val if pd.notna(ic_val) else 0.0:.4f}")
        c5.metric("ICIR", f"{icir if (icir is not None and not np.isnan(icir)) else 0.0:.2f}")

    # å·¦å³ï¼šå‡€å€¼å¯¹æ¯” + æœˆåº¦çƒ­åŠ›å›¾
    left, right = st.columns(2)
    with left:
        st.subheader("ç­–ç•¥ vs æ²ªæ·±300")
        try:
            if not df_log.empty:
                ret = df_log["real_ret"].fillna(0)
                eq = (1+ret).cumprod()
                start, end = df_log["next_day"].min(), df_log["next_day"].max()
                bench = fetch_daily("000300.SH", str(start), str(end))
                fig, ax = plt.subplots(figsize=(8,4))
                ax.plot(eq.values, label="ç­–ç•¥å‡€å€¼")
                if bench is not None and not bench.empty:
                    b = bench.set_index("trade_date")["close"].pct_change().fillna(0)
                    beq = (1+b).cumprod()
                    ax.plot(beq.values[:len(eq)], label="æ²ªæ·±300")
                ax.legend(); ax.set_title("å‡€å€¼å¯¹æ¯”")
                st.pyplot(fig)
            else:
                st.info("æ— æ—¥å¿—æ•°æ®ã€‚")
        except Exception as e:
            st.warning(f"ç»˜å›¾å¤±è´¥ï¼š{e}")

    with right:
        st.subheader("æœˆåº¦çƒ­åŠ›å›¾")
        try:
            if not df_log.empty:
                d = df_log.copy()
                d["year"] = d["next_day"].astype(str).str[:4]
                d["month"] = d["next_day"].astype(str).str[4:6].astype(int)
                pv = d.groupby(["year","month"])["real_ret"].mean().reset_index().pivot("year","month","real_ret")
                fig, ax = plt.subplots(figsize=(10,4))
                sns.heatmap(pv, annot=True, fmt=".2%", cmap="RdYlGn", center=0, ax=ax)
                ax.set_title("æœˆåº¦å¹³å‡æ”¶ç›Šçƒ­åŠ›å›¾")
                st.pyplot(fig)
            else:
                st.info("æ— æ—¥å¿—æ•°æ®ã€‚")
        except Exception as e:
            st.warning(f"ç»˜åˆ¶çƒ­åŠ›å›¾å¤±è´¥ï¼š{e}")

    # IC æ—¶é—´åºåˆ—
    st.subheader("ğŸ“‰ æ¯æ—¥ IC èµ°åŠ¿ï¼ˆè®­ç»ƒçª—å£å†…ï¼‰")
    if os.path.exists(ic_series_file):
        ic_df = pd.read_csv(ic_series_file)
        if not ic_df.empty:
            fig, ax = plt.subplots(figsize=(9,4))
            x = pd.to_datetime(ic_df["trade_date"])
            ax.plot(x, ic_df["ic"], marker="o", linewidth=1)
            ax.axhline(0, color="black", linestyle="--")
            ax.set_title("æ¯æ—¥IC")
            st.pyplot(fig)
        else:
            st.info("æš‚æ—  IC åºåˆ—æ•°æ®ã€‚")
    else:
        st.info("æœªæ‰¾åˆ° ic_series.csvï¼Œè¯·åœ¨ä¾§æ å…ˆè¿è¡Œåå°æµç¨‹ã€‚")

    # æœ€æ–°é¢„æµ‹
    st.subheader("ğŸ“‹ æœ€æ–°é¢„æµ‹ TopK")
    preds_files = [f for f in os.listdir(DATA_DIR) if f.startswith("predictions_") and f.endswith(".csv")]
    if preds_files:
        preds_files.sort()
        latest_file = preds_files[-1]
        df_pred = pd.read_csv(os.path.join(DATA_DIR, latest_file))
        show_cols = [c for c in ["ts_code","strong_prob","untrad_prob","ret_pred_raw","ret_pred"] if c in df_pred.columns]
        st.caption(f"æ–‡ä»¶ï¼š{latest_file}")
        st.dataframe(df_pred[show_cols].sort_values("ret_pred", ascending=False).head(TOPK), use_container_width=True)
    else:
        st.info("æœªå‘ç° predictions_*.csv æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œåå°æµç¨‹ã€‚")

    # å¸‚åœºçƒ­ç‚¹
    st.subheader("ğŸ”¥ å¸‚åœºçƒ­ç‚¹ Top5")
    ind_top_path = os.path.join(DATA_DIR, "industry_top5.csv")
    con_top_path = os.path.join(DATA_DIR, "concept_top5.csv")
    cols = st.columns(2)
    with cols[0]:
        st.markdown("**è¡Œä¸šå¼ºåº¦ Top5**")
        if os.path.exists(ind_top_path):
            ind_top = pd.read_csv(ind_top_path)
            st.dataframe(ind_top, use_container_width=True)
        else:
            st.info("æš‚æ— è¡Œä¸šTop5ã€‚")
    with cols[1]:
        st.markdown("**æ¦‚å¿µå¼ºåº¦ Top5**")
        if os.path.exists(con_top_path):
            con_top = pd.read_csv(con_top_path)
            st.dataframe(con_top, use_container_width=True)
        else:
            st.info("æš‚æ— æ¦‚å¿µTop5ï¼ˆå¯èƒ½å› ç½‘ç»œ/æ˜ å°„æ— æ³•è·å–ï¼‰ã€‚")

    # æ‰§è¡Œå±‚å¤šå‘¨æœŸå±•ç¤ºï¼ˆç›´æ¥è¯»å–å·²ç”Ÿæˆçš„å›¾ç‰‡ï¼‰
    st.subheader("ğŸ§ª æ‰§è¡Œå±‚å‡€å€¼æ›²çº¿ï¼ˆT+0 / 2 / 3 / 5ï¼‰")
    imgs = []
    for d in EXEC_HOLD_DAYS_LIST:
        p = os.path.join(DATA_DIR, f"execution_hold{d}.png")
        if os.path.exists(p): imgs.append(p)
    if imgs:
        for p in imgs:
            st.image(p, caption=os.path.basename(p), use_container_width=True)
    else:
        st.info("æœªæ‰¾åˆ°æ‰§è¡Œå±‚æ›²çº¿å›¾ç‰‡ï¼Œè¯·å…ˆè¿è¡Œåå°æµç¨‹ã€‚")

    # å•è‚¡è·Ÿè¸ª
    st.subheader("ğŸ” å•è‚¡è·Ÿè¸ª")
    if not df_log.empty:
        codes = sorted(df_log["ts_code"].unique())
        sel = st.selectbox("é€‰æ‹©è‚¡ç¥¨", options=codes)
        if sel:
            d = df_log[df_log["ts_code"]==sel].copy()
            d["date"] = pd.to_datetime(d["next_day"], format="%Y%m%d")
            fig, ax = plt.subplots(figsize=(8,4))
            if "ret_pred" in d.columns:
                ax.plot(d["date"], d["ret_pred"], marker="o", label="é¢„æµ‹æ”¶ç›Š")
            ax.plot(d["date"], d["real_ret"], marker="x", label="å®é™…æ”¶ç›Š")
            ax.axhline(0, color="black", linestyle="--")
            ax.set_title(f"{sel} é¢„æµ‹ vs å®é™…æ”¶ç›Š"); ax.legend()
            st.pyplot(fig)
    else:
        st.info("æš‚æ— æ—¥å¿—æ•°æ®ï¼Œæ— æ³•å±•ç¤ºå•è‚¡è·Ÿè¸ªã€‚")

    st.success("âœ… çœ‹æ¿å°±ç»ªã€‚ä¾§æ å¯éšæ—¶é‡æ–°è¿è¡Œåå°æµç¨‹ã€‚")

# ================= ä¸»å…¥å£ =================
def main():
    # æ™®é€š python è¿è¡Œï¼šåªè·‘åå°
    if not STREAMLIT_AVAILABLE or ("streamlit" not in sys.argv[0].lower() and "streamlit" not in " ".join(sys.argv).lower()):
        run_backend_once()
    else:
        # Streamlit ç¯å¢ƒï¼šå…ˆè·‘åå°ä¸€æ¬¡ï¼Œå†è¿›å…¥çœ‹æ¿
        run_backend_once()
        run_streamlit_ui()

if __name__=="__main__":
    main()
